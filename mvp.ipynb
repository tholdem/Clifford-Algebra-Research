{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from external.metric import ShortLexBasisBladeOrder, construct_gmt, gmt_element\n",
    "from external.mvsilu import MVSiLU\n",
    "#from external.mvlayernorm import MVLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = [1,1]\n",
    "d = len(metric)\n",
    "N = 1024 # sample size\n",
    "# sample two vectors u,v per sample\n",
    "x = torch.randn(N,2,d)\n",
    "y = torch.randint(1,3,(N,)) # sample 1 or 2 for y, a categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(u, v, y):= \\begin{cases}\\cos(\\lVert u \\rVert) &y=1 \\\\\n",
    "\\frac{\\langle u, v \\rangle ^3}{10} \\qquad &y=2\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the target values\n",
    "f = torch.zeros(N)\n",
    "metric_t = torch.tensor(metric,dtype=torch.float) # convert metric to tensor type\n",
    "f[y==1] = torch.cos(torch.einsum(\"bi,i,bi->b\",x[y==1][:,0],metric_t,x[y==1][:,0]).abs().sqrt()) # compute u norm via u^TMu\n",
    "f[y==2] = 1/10 * torch.einsum(\"bi,i,bi->b\",x[y==2][:,0],metric_t,x[y==2][:,1])**3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffordAlgebra(nn.Module):\n",
    "    def __init__(self, metric):\n",
    "        super().__init__()\n",
    "        # include metric as a tensor to be part of state but won't be updated by optimizer\n",
    "        self.register_buffer(\"metric\", torch.as_tensor(metric)) # signature of symmetric bilinear form\n",
    "        self.vspace_dim = len(metric) # vector space dimension = number of vector basis\n",
    "        self.bbo = ShortLexBasisBladeOrder(self.vspace_dim) # get ordered algebra basis (external)\n",
    "        self.algebra_dim = len(self.bbo.grades) # algebra dimension\n",
    "        # construct cayley table (external)\n",
    "        cayley = (\n",
    "            construct_gmt(\n",
    "                self.bbo.index_to_bitmap, self.bbo.bitmap_to_index, self.metric\n",
    "            )\n",
    "            .to_dense()\n",
    "            .to(torch.get_default_dtype())\n",
    "        )\n",
    "        self.register_buffer(\"cayley\", cayley)\n",
    "        self.grades = self.bbo.grades.unique() # all grade numbers (e.g. [0,1,2,3] for n = 3) as a tensor\n",
    "        self.register_buffer(\"subspaces\",torch.as_tensor([math.comb(self.vspace_dim, grade) for grade in self.grades])) # [1 3 3 1] for n = 3\n",
    "        self.n_subspaces = len(self.subspaces)\n",
    "        self.grade2slice = self._grade2slice()\n",
    "        \n",
    "\n",
    "    def geometric_product(self, x, y, ):\n",
    "        cayley = self.cayley\n",
    "        return torch.einsum(\"...i, ikj, ...j -> k\", x, cayley, y)\n",
    "    \n",
    "    def embed_grade(self, tensor: torch.Tensor, grade: int) -> torch.Tensor:\n",
    "        mv = torch.zeros(*tensor.shape[:-1], 2**self.vspace_dim, device=tensor.device)\n",
    "        s = self.grade2slice[grade]\n",
    "        mv[..., s] = tensor \n",
    "        return mv\n",
    "\n",
    "    \n",
    "    def _grade2slice(self):\n",
    "        grade2slice = list()\n",
    "        #convert subspaces data (a list of counts of subspaces with increasing dimension) to tensor e.g. [1 3 3 1] for n=3\n",
    "        subspaces = torch.as_tensor(self.subspaces)\n",
    "        for grade in self.grades:\n",
    "            index_start = subspaces[:grade].sum()\n",
    "            index_end = index_start + math.comb(self.vspace_dim, grade)\n",
    "            grade2slice.append(slice(index_start, index_end))\n",
    "        return grade2slice\n",
    "\n",
    "    \n",
    "    def geometric_product_paths(self):\n",
    "        # dim+1 since we have scalars as 0-dim\n",
    "        gp_paths = torch.zeros((self.vspace_dim + 1, self.vspace_dim + 1, self.vspace_dim + 1), dtype=bool)\n",
    "        # \n",
    "        for i in range(self.vspace_dim + 1):\n",
    "            for j in range(self.vspace_dim + 1):\n",
    "                for k in range(self.vspace_dim + 1):\n",
    "                    s_i = self.grade2slice[i]\n",
    "                    s_j = self.grade2slice[j]\n",
    "                    s_k = self.grade2slice[k]\n",
    "                    \n",
    "                    # m is a 3D tensor, capturing whether basis of subspaces of two grades give any basis of the third grade\n",
    "                    # e.g. i = 2, j = 2, k = 1, do basis (e12, e23, e13) and (e12, e23, e13) give any 1-blade basis? No. So\n",
    "                    # gp_paths(2,2,1) = False/0\n",
    "                    m = self.cayley[s_i, s_j, s_k]\n",
    "                    gp_paths[i, j, k] = (m != 0).any()\n",
    "\n",
    "        return gp_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVLinear(nn.Module):\n",
    "    def __init__(self, algebra, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.algebra = algebra\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = nn.Parameter(torch.empty(out_features,in_features,algebra.n_subspaces))\n",
    "        self.normalize_parameters()\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        torch.nn.init.normal_(self.weights, std = 1/ math.sqrt(self.in_features))\n",
    "\n",
    "    # we have l number of input channels (batch size)\n",
    "    def forward(self, input):\n",
    "        weights = self.weights.repeat_interleave(self.algebra.subspaces,dim=-1) #repeat along the last dimension (n_subspaces) of weights\n",
    "        return torch.einsum(\"bm...i,nmi -> bn...i\", input, weights) #output for each batch b, output feature n, and subspace i, ... represents shape of a single datapoint\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedGeometricProductLayer(nn.Module):\n",
    "    def __init__(self, algebra, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.algebra = algebra\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.linear_right = MVLinear(algebra, in_features, in_features)\n",
    "        self.linear_left = MVLinear(algebra,in_features,out_features)\n",
    "\n",
    "        self.normalization = nn.Identity() #TODO\n",
    "\n",
    "        self.product_paths = algebra.geometric_product_paths()\n",
    "        self.weights = nn.Parameter(torch.empty(out_features,in_features,self.product_paths.sum()))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal(self.weights, \n",
    "                       std = 2/math.sqrt(self.in_features * (self.algebra.vspace_dim + 1)) #for ReLU, we want total input features, have one for each grade)\n",
    "                       )\n",
    "        \n",
    "    def _get_weights(self):\n",
    "        weights = torch.zeros(\n",
    "            self.out_features, self.in_features, *self.product_paths.size(),dtype=self.weights.dtype , device=self.weights.device\n",
    "        )\n",
    "        subspaces = self.algebra.subspaces\n",
    "        weights[:,:,self.product_paths] = self.weights\n",
    "        weights_repeated = weights.repeat_interleave(subspaces, dim = -3).repeat_interleave(subspaces, dim = -2). repeat_interleave(subspaces, dim = -1)\n",
    "        return self.algebra.cayley * weights_repeated #get sparse weights\n",
    "    \n",
    "    def forward(self,input):\n",
    "        input_right = self.linear_right(input)\n",
    "        input_right = self.normalization(input_right)\n",
    "        weights = self._get_weights()\n",
    "        return (self.linear_left(input) + torch.einsum(\"bni, mnijk, bnk -> bmj\",input, weights, input_right))/ math.sqrt(2) # eq 15 and 14, i + k = j, more normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGEBlock(nn.Module):\n",
    "    def __init__(self, algebra, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MVLinear(algebra,in_features,out_features),\n",
    "            #MVSiLU(algebra,out_features),\n",
    "            FullyConnectedGeometricProductLayer(algebra,out_features,out_features),\n",
    "            #MVLayerNorm(algebra,out_features)\n",
    "        )\n",
    "    def forward(self,input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGEMLP(nn.Module):\n",
    "    def __init__(self, algebra, in_features, hidden_features, out_features, n_layers = 2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_layers-1):\n",
    "            layers.append(CGEBlock(algebra,in_features,hidden_features))\n",
    "            in_features = hidden_features\n",
    "        layers.append(CGEBlock(algebra,hidden_features,out_features))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    def forward(self,input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantCGENN(nn.Module):\n",
    "    def __init__(self, algebra, in_features, hidden_features, out_features, n_layers = 2):\n",
    "        super().__init__()\n",
    "        self.hidden_features = hidden_features\n",
    "        self.cgemlp = CGEMLP(algebra, in_features, hidden_features, hidden_features, n_layers=n_layers)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_features,hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features,out_features),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self,input):\n",
    "        h = self.cgemlp(input)\n",
    "        return self.mlp(h[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2, 4])\n",
      "tensor([[ 0.0000, -1.2536, -0.4742,  0.0000],\n",
      "        [ 0.0000,  0.8593, -0.5787,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "ca = CliffordAlgebra(metric)\n",
    "x_cl = ca.embed_grade(x,1)\n",
    "print(x_cl.shape)\n",
    "print(x_cl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        ...,\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oh = F.one_hot(y - 1, 2)\n",
    "y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2, 4])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y_cl = ca.embed_grade(y_oh[..., None], 0)\n",
    "print(y_cl.shape)\n",
    "print(y_cl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cl = torch.cat([x_cl, y_cl], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bryan\\AppData\\Local\\Temp\\ipykernel_15208\\30745984.py:19: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(self.weights,\n"
     ]
    }
   ],
   "source": [
    "model = InvariantCGENN(ca,4,32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37313 parameters.\n",
      "\n",
      "Step: 0. Loss: 3.01\n",
      "Step: 4. Loss: 2.11\n",
      "Step: 8. Loss: 2.10\n",
      "Step: 12. Loss: 2.08\n",
      "Step: 16. Loss: 2.06\n",
      "Step: 20. Loss: 2.03\n",
      "Step: 24. Loss: 2.01\n",
      "Step: 28. Loss: 2.01\n",
      "Step: 32. Loss: 2.00\n",
      "Step: 36. Loss: 2.00\n",
      "Step: 40. Loss: 2.00\n",
      "Step: 44. Loss: 2.00\n",
      "Step: 48. Loss: 2.00\n",
      "Step: 52. Loss: 2.00\n",
      "Step: 56. Loss: 2.00\n",
      "Step: 60. Loss: 2.00\n",
      "Step: 64. Loss: 2.00\n",
      "Step: 68. Loss: 2.00\n",
      "Step: 72. Loss: 2.00\n",
      "Step: 76. Loss: 2.00\n",
      "Step: 80. Loss: 2.00\n",
      "Step: 84. Loss: 2.00\n",
      "Step: 88. Loss: 2.00\n",
      "Step: 92. Loss: 2.00\n",
      "Step: 96. Loss: 2.00\n",
      "Step: 100. Loss: 2.00\n",
      "Step: 104. Loss: 2.00\n",
      "Step: 108. Loss: 2.00\n",
      "Step: 112. Loss: 2.00\n",
      "Step: 116. Loss: 2.00\n",
      "Step: 120. Loss: 2.00\n",
      "Step: 124. Loss: 2.00\n",
      "Step: 128. Loss: 2.00\n",
      "Step: 132. Loss: 2.00\n",
      "Step: 136. Loss: 2.00\n",
      "Step: 140. Loss: 2.00\n",
      "Step: 144. Loss: 2.00\n",
      "Step: 148. Loss: 2.00\n",
      "Step: 152. Loss: 2.00\n",
      "Step: 156. Loss: 2.00\n",
      "Step: 160. Loss: 2.00\n",
      "Step: 164. Loss: 2.00\n",
      "Step: 168. Loss: 2.00\n",
      "Step: 172. Loss: 2.00\n",
      "Step: 176. Loss: 2.00\n",
      "Step: 180. Loss: 2.00\n",
      "Step: 184. Loss: 2.00\n",
      "Step: 188. Loss: 2.00\n",
      "Step: 192. Loss: 2.00\n",
      "Step: 196. Loss: 2.00\n",
      "Step: 200. Loss: 2.00\n",
      "Step: 204. Loss: 2.00\n",
      "Step: 208. Loss: 2.00\n",
      "Step: 212. Loss: 2.00\n",
      "Step: 216. Loss: 2.00\n",
      "Step: 220. Loss: 2.00\n",
      "Step: 224. Loss: 2.00\n",
      "Step: 228. Loss: 2.00\n",
      "Step: 232. Loss: 2.00\n",
      "Step: 236. Loss: 2.00\n",
      "Step: 240. Loss: 2.00\n",
      "Step: 244. Loss: 2.00\n",
      "Step: 248. Loss: 2.00\n",
      "Step: 252. Loss: 2.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model has {sum(p.numel() for p in model.parameters())} parameters.\\n\")\n",
    "adam = optim.Adam(model.parameters())\n",
    "\n",
    "for i in range(256):\n",
    "\n",
    "    output = model(input_cl)\n",
    "    loss = F.mse_loss(output.squeeze(-1), f)\n",
    "\n",
    "    adam.zero_grad()\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "\n",
    "    if i % 4 == 0:\n",
    "        print(f\"Step: {i}. Loss: {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

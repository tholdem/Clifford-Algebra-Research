{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from external.metric import ShortLexBasisBladeOrder, construct_gmt, gmt_element\n",
    "from external.mvsilu import MVSiLU\n",
    "#from external.mvlayernorm import MVLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = [1,1]\n",
    "d = len(metric)\n",
    "N = 1024 # sample size\n",
    "# sample two vectors u,v per sample\n",
    "x = torch.randn(N,2,d)\n",
    "y = torch.randint(1,3,(N,)) # sample 1 or 2 for y, a categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(u, v, y):= \\begin{cases}\\cos(\\lVert u \\rVert) &y=1 \\\\\n",
    "\\frac{\\langle u, v \\rangle ^3}{10} \\qquad &y=2\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the target values\n",
    "f = torch.zeros(N)\n",
    "metric_t = torch.tensor(metric,dtype=torch.float) # convert metric to tensor type\n",
    "f[y==1] = torch.cos(torch.einsum(\"bi,i,bi->b\",x[y==1][:,0],metric_t,x[y==1][:,0]).abs().sqrt()) # compute u norm via u^TMu\n",
    "f[y==2] = 1/10 * torch.einsum(\"bi,i,bi->b\",x[y==2][:,0],metric_t,x[y==2][:,1])**3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffordAlgebra(nn.Module):\n",
    "    def __init__(self, metric):\n",
    "        super().__init__()\n",
    "        # include metric as a tensor to be part of state but won't be updated by optimizer\n",
    "        self.register_buffer(\"metric\", torch.as_tensor(metric)) # signature of symmetric bilinear form\n",
    "        self.vspace_dim = len(metric) # vector space dimension = number of vector basis\n",
    "        self.bbo = ShortLexBasisBladeOrder(self.vspace_dim) # get ordered algebra basis (external)\n",
    "        self.algebra_dim = len(self.bbo.grades) # algebra dimension\n",
    "        # construct cayley table (external)\n",
    "        cayley = (\n",
    "            construct_gmt(\n",
    "                self.bbo.index_to_bitmap, self.bbo.bitmap_to_index, self.metric\n",
    "            )\n",
    "            .to_dense()\n",
    "            .to(torch.get_default_dtype())\n",
    "        )\n",
    "        self.register_buffer(\"cayley\", cayley)\n",
    "        self.grades = self.bbo.grades.unique() # all grade numbers (e.g. [0,1,2,3] for n = 3) as a tensor\n",
    "        self.register_buffer(\"subspaces\",torch.as_tensor([math.comb(self.vspace_dim, grade) for grade in self.grades])) # [1 3 3 1] for n = 3\n",
    "        self.n_subspaces = len(self.subspaces)\n",
    "        self.grade2slice = self._grade2slice()\n",
    "        \n",
    "\n",
    "    def geometric_product(self, x, y, ):\n",
    "        cayley = self.cayley\n",
    "        return torch.einsum(\"...i, ikj, ...j -> k\", x, cayley, y)\n",
    "    \n",
    "    def embed_grade(self, tensor: torch.Tensor, grade: int) -> torch.Tensor:\n",
    "        mv = torch.zeros(*tensor.shape[:-1], 2**self.vspace_dim, device=tensor.device)\n",
    "        s = self.grade2slice[grade]\n",
    "        mv[..., s] = tensor \n",
    "        return mv\n",
    "\n",
    "    \n",
    "    def _grade2slice(self):\n",
    "        grade2slice = list()\n",
    "        #convert subspaces data (a list of counts of subspaces with increasing dimension) to tensor e.g. [1 3 3 1] for n=3\n",
    "        subspaces = torch.as_tensor(self.subspaces)\n",
    "        for grade in self.grades:\n",
    "            index_start = subspaces[:grade].sum()\n",
    "            index_end = index_start + math.comb(self.vspace_dim, grade)\n",
    "            grade2slice.append(slice(index_start, index_end))\n",
    "        return grade2slice\n",
    "\n",
    "    \n",
    "    def geometric_product_paths(self):\n",
    "        # dim+1 since we have scalars as 0-dim\n",
    "        gp_paths = torch.zeros((self.vspace_dim + 1, self.vspace_dim + 1, self.vspace_dim + 1), dtype=bool)\n",
    "        # \n",
    "        for i in range(self.vspace_dim + 1):\n",
    "            for j in range(self.vspace_dim + 1):\n",
    "                for k in range(self.vspace_dim + 1):\n",
    "                    s_i = self.grade_to_slice[i]\n",
    "                    s_j = self.grade_to_slice[j]\n",
    "                    s_k = self.grade_to_slice[k]\n",
    "                    \n",
    "                    # m is a 3D tensor, capturing whether basis of subspaces of two grades give any basis of the third grade\n",
    "                    # e.g. i = 2, j = 2, k = 1, do basis (e12, e23, e13) and (e12, e23, e13) give any 1-blade basis? No. So\n",
    "                    # gp_paths(2,2,1) = False/0\n",
    "                    m = self.cayley[s_i, s_j, s_k]\n",
    "                    gp_paths[i, j, k] = (m != 0).any()\n",
    "\n",
    "        return gp_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVLinear(nn.Module):\n",
    "    def __init__(self, algebra, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.algebra = algebra\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = nn.Parameter(torch.empty(out_features,in_features,algebra.n_subspaces))\n",
    "        self.normalize_parameters()\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        torch.nn.init.normal_(self.weights, std = 1/ math.sqrt(self.in_features))\n",
    "\n",
    "    # we have l number of input channels (batch size)\n",
    "    def forward(self, input):\n",
    "        weights = self.weights.repeat_interleave(self.algebra.subspaces,dim=-1) #repeat along the last dimension (n_subspaces) of weights\n",
    "        return torch.einsum(\"bm...i,nmi -> bn...i\", input, weights) #output for each batch b, output feature n, and subspace i, ... represents shape of a single datapoint\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedGeometricProductLayer(nn.Module):\n",
    "    def __init__(self, algebra, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.algebra = algebra\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.linear_right = MVLinear(algebra, in_features, in_features)\n",
    "        self.linear_left = MVLinear(algebra,in_features,out_features)\n",
    "\n",
    "        self.normalization = nn.Identity()\n",
    "\n",
    "        self.product_paths = algebra.geometric_product_paths\n",
    "        self.weights = nn.Parameter(torch.empty(out_features,in_features,self.product_paths.sum()))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal(self.weights, \n",
    "                       std = 2/math.sqrt(self.in_features * (self.algebra.vspace_dim + 1)) #for ReLU, we want total input features, have one for each grade)\n",
    "                       )\n",
    "        \n",
    "    def _get_weights(self):\n",
    "        weights = torch.zeros(\n",
    "            self.out_features, self.in_features, *self.product_paths.size(),dtype=self.weights.dtype , device=self.weights.device\n",
    "        )\n",
    "        subspaces = self.algebra.subspaces\n",
    "        weights[:,:,self.product_paths] = self.weights\n",
    "        weights_repeated = weights.repeat_interleave(subspaces, dim = -3).repeat_interleave(subspaces, dim = -2). repeat_interleave(subspaces, dim = -1)\n",
    "        return self.algebra.cayley * weights_repeated\n",
    "    \n",
    "    def forward(self,input):\n",
    "        input_right = self.linear_right(input)\n",
    "        input_right = self.normalization(input_right)\n",
    "        weights = self._get_weights()\n",
    "        return (self.linear_left(input) + torch.einsum(\"bni, mnijk, bnk -> bmj\",input, weights, input_right))/ math.sqrt(2) # eq 15 and 14, i + k = j, more normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGEBlock(nn.Module):\n",
    "    def __init__(self, algebra, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MVLinear(algebra,in_features,out_features)#,\n",
    "            #MVSiLU(algebra,out_features),\n",
    "            #MVLayerNorm(algebra,out_features)\n",
    "        )\n",
    "    def forward(self,input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGEMLP(nn.Module):\n",
    "    def __init__(self, algebra, in_features, hidden_features, out_features, n_layers = 2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_layers-1):\n",
    "            layers.append(CGEBlock(algebra,in_features,hidden_features))\n",
    "            in_features = hidden_features\n",
    "        layers.append(CGEBlock(algebra,hidden_features,out_features))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    def forward(self,input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2, 4])\n",
      "tensor([[ 0.0000,  0.3261,  0.0647,  0.0000],\n",
      "        [ 0.0000, -0.7778,  0.2760,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "ca = CliffordAlgebra(metric)\n",
    "x_cl = ca.embed_grade(x,1)\n",
    "print(x_cl.shape)\n",
    "print(x_cl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        ...,\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oh = F.one_hot(y - 1, 2)\n",
    "y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2, 4])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y_cl = ca.embed_grade(y_oh[..., None], 0)\n",
    "print(y_cl.shape)\n",
    "print(y_cl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cl = torch.cat([x_cl, y_cl], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CGEMLP(ca,4,32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 480 parameters.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bryan\\AppData\\Local\\Temp\\ipykernel_24000\\524895709.py:7: UserWarning: Using a target size (torch.Size([1024])) that is different to the input size (torch.Size([1024, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(output.squeeze(-2), f)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (1024) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m      6\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(input_cl)\n\u001b[1;32m----> 7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     adam\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\Github\\Clifford-Algebra-Research\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3365\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3363\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3365\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32md:\\Github\\Clifford-Algebra-Research\\.venv\\Lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (1024) at non-singleton dimension 1"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"The model has {sum(p.numel() for p in model.parameters())} parameters.\\n\")\n",
    "adam = optim.Adam(model.parameters())\n",
    "\n",
    "for i in range(256):\n",
    "\n",
    "    output = model(input_cl)\n",
    "    loss = F.mse_loss(output.squeeze(-1), f)\n",
    "\n",
    "    adam.zero_grad()\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "\n",
    "    if i % 4 == 0:\n",
    "        print(f\"Step: {i}. Loss: {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
